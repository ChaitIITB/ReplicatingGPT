<div align="center">

# Replicating GPTs

</div>
This is an implementation of GPT-1 and GPT 2 and GPT-3 in a very basic way.

## Features

GPT1.py is an implementation of a character level tokenizer after which there is some of the scaled dot-product attention used from the famous paper [attention is all you need](https://doi.org/10.48550/arXiv.1706.03762).

GPT-2.py is still an incomplete file and will be completed by the end of summer [I hope].

## Quick Start

To run the the GPT - 1 implementation just input the following after downloading or writing the script in each of the file
```bash
python GPT1.py
```

## Credits 

Stuff is taken from andrej karpathy's video lecutures and replicated. 

